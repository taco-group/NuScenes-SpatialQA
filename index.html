<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>NuScenes-SpatialQA</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <img 
                src="static/images/logo3.png"
                class="center"
                width="180"
              />
            <h1 class="title is-1 publication-title">NuScenes-SpatialQA: <br> A Spatial Understanding and Reasoning Benchmark for Vision-Language Models in Autonomous Driving
            </h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Kexin Tian</a><sup>1</sup>,</span>
                <span class="author-block">
                  Jingrui Mao</a><sup>1</sup>,</span>
                  <span class="author-block">
                    Yunlong Zhang</a><sup>1</sup>,
                  </span>
                  <span class="author-block">
                    Jiwan Jiang</a><sup>2</sup>,
                  </span>
                  <span class="author-block">
                    Yang Zhou</a><sup>1,&#8225; </sup>,
                  </span><br>
                  <span class="author-block">
                    Zhengzhong Tu</a><sup>1,&#8225;</sup>
                  </span>
                  </div>
                  <div>
                    <sup>&#8225;</sup> Corresponding authors.
                  </div>
                  

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Texas A&M University, <sup>2</sup>University of Wisconsin - Madison<br>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://huggingface.co/datasets/ktian6/NuScenes-SpatialQA" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-database"></i>
                    </span>
                    <span>Benchmark</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://github.com/taco-group/NuScenes-SpatialQA" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4" -->
        <!-- type="video/mp4"> -->
      <!-- </video> -->
      <img src="./static/images/radar.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Comprehensive experiments on our NuScenes-SpatialQA benchmark have demonstrated VLMs' performance on spatial understanding and reasoning abilities, including qualitative spatial relationship tasks (left) and quantitative spatial measurement tasks (right).
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Recent advancements in Vision-Language Models (VLMs) have demonstrated strong potential for autonomous driving tasks. However, their spatial understanding and reasoning—key capabilities for autonomous driving—still exhibit significant limitations. Notably, none of the existing benchmarks systematically evaluate VLMs' spatial reasoning capabilities in driving scenarios. To fill this gap, we propose <b>NuScenes-SpatialQA</b>, the first large-scale ground-truth-based Question-Answer (QA) benchmark specifically designed to evaluate the spatial understanding and reasoning capabilities of VLMs in autonomous driving. Built upon the NuScenes dataset, the benchmark is constructed through an automated 3D scene graph generation pipeline and a QA generation pipeline. The benchmark systematically evaluates VLMs' performance in both spatial understanding and reasoning across multiple dimensions. Using this benchmark, we conduct extensive experiments on diverse VLMs, including both general and spatial-enhanced models, providing the first comprehensive evaluation of their spatial capabilities in autonomous driving. Surprisingly, the experimental results show that the spatial-enhanced VLM outperforms in qualitative QA but does not demonstrate competitiveness in quantitative QA. In general, VLMs still face considerable challenges in spatial understanding and reasoning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">QA Generation Pipeline</h2>
      <div class="content has-text-justified">
        <p>
            Our QA generation framework involves two key pipelines: 
            <strong>①<u>3D Scene Graph Generation</u></strong>: We construct a 3D scene graph for each camera view in the NuScenes dataset. An <u>auto-captioning</u> process is designed to generate instance-level descriptions for each annotated object, and the generated captions are combined with selected 3D attributes from NuScenes as node attributes. Spatial relationships between objects are encoded as edge attributes.       
            <strong>②<u>Q&A Pairs Generation</u></strong>: Based on the structured scene graph and our predefined QA templates, we generate multi-aspect QA pairs that comprehensively cover both spatial understanding and spatial reasoning, providing a holistic evaluation of the spatial capabilities of VLMs.
            An overview of each step is presented in the following figure.
        </p>
      </div>
      <div class="content has-text-centered">
        <img
          src="./static/images/framework.png"
          class="inline-figure-six"
          alt="QA Generation Framework"
          style="width: 88%; height: auto;"
        />
      </div>
      <h3 class="title is-4">QA Example</h3>
      <div class="content has-text-justified">
        <p>
          The final QA pairs are designed to comprehensively evaluate the spatial capabilities of vision-language models, and are categorized into two main types:
        </p>
        <ol>
            <li>
              <b>Spatial Understanding</b>: Assesses direct spatial relationships and metric measurements, including:
              <ul>
                <li><u><em>Qualitative QA</em></u>: Tasks that evaluate relative spatial relations, like relative spatial relationships and dimension comparison.</li>
                <li><u><em>Quantitative QA</em></u>: Tasks that involve direct numerical estimation, requiring models to extract specific values such as distances, dimensions, or angles.</li>
              </ul>
            </li>
            <li>
              <b>Spatial Reasoning</b>: Involves higher-level inference beyond direct attribute retrieval, including:
              <ul>
                <li><u><em>Direct Reasoning QA</em></u>: Deductive questions based on object relations.</li>
                <li><u><em>Situational Reasoning QA</em></u>: Real-world scenario-based reasoning involving safety or physical constraints.</li>
              </ul>
            </li>
        </ol>
        <p>
            The figure below shows some examples of the QA pairs:
        </p>
        </div>
            <div class="content has-text-centered">
            <img
                src="./static/images/example.png"
                class="inline-figure-six"
                alt="QA Example."
            />
        </div>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Benchmark Statistics</h2>
      <div class="content has-text-justified">
        <p>
          The final benchmark consists of approximately <strong>3.5 million</strong> QA pairs, including around <strong>2.5M</strong> qualitative and <strong>0.6M</strong> quantitative questions under the spatial understanding category, as well as <strong>0.2M</strong> reasoning-based questions covering both direct and situational reasoning.
        </p>
        <p>
          These QA pairs span <strong>6,000</strong> keyframes, each captured from <strong>6 camera views</strong> in the NuScenes dataset.
        </p>
        <p>
          As shown in the following figure, we compare NuScenes-SpatialQA with existing open-source benchmarks in autonomous driving and spatial reasoning to highlight its scale and coverage. It is the first <u>large-scale</u>, <u>ground-truth-based</u> QA benchmark specifically designed to evaluate both <u>spatial understanding</u> and <u>spatial reasoning</u> capabilities of VLMs in <u>autonomous driving</u>.
        </p>
      </div>
      <div class="content has-text-centered"> 
          <img
            src="./static/images/compare_table.png"
            class="inline-figure-six"
            alt="Benchmark Comparison Table."
            style="width: 98%; max-width: 900px; height: auto;"
          />
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop content">
      <h2 class="title is-3">Evaluation Results</h2>
      <figure style="text-align: center;">
        <img src="./static/images/table1.png" alt="Benchmark results on spatial understanding tasks." style="max-width: 100%; height: auto;" />
        <figcaption style="display: inline-block; text-align: left; font-size: 1.0em; color: #555; margin-top: 8px; max-width: 988px;">
          Performance on <strong>spatial understanding</strong> tasks in NuScenes-SpatialQA. The upper part of the table reports results on <strong>Qualitative Spatial QA</strong>, where values represent <em>accuracy</em> (<strong>↑</strong>). The lower part presents results on <strong>Quantitative Spatial QA</strong>, where values correspond to <em title="A prediction is considered correct if it falls within a predefined numeric threshold.">Tolerance-based Accuracy *</em> (<strong>↑</strong>) / <em>MAE</em> (<strong>↓</strong>). Baseline marked with <strong>✧</strong> is spatial-enhanced VLM.    
          <br />
          <em style="font-size: 0.9em;">* Tolerance-based Accuracy is defined to measure the proportion of model responses that fall within the range of [75%, 125%] of the ground-truth answer.      </em>
        </figcaption>
      </figure>       

        <div style="height: 16px;"></div>

        <!-- <h3 class="title is-4">Spatial Reasoning</h3> -->
        <figure style="text-align: center;">
            <img src="./static/images/table2.png" alt="Benchmark results on spatial reasoning tasks." style="max-width: 88%; height: auto;" />
            <figcaption style="display: inline-block; text-align: left; font-size: 1.0em; color: #555; margin-top: 8px; max-width: 988px;">
            Performance on <strong>Spatial Reasoning</strong> tasks in NuScenes-SpatialQA. The table reports <em>Tolerance-based Accuracy</em> (<strong>↑</strong>) across different VLMs.         
            </figcaption>
        </figure>
        <div style="height: 16px;"></div>

        <!-- <h3 class="title is-4">Effect of Backbone Architecture</h3> -->
        <figure style="text-align: center;">
            <img src="./static/images/table3.png" alt="Ablation study on backbone and scaling." style="max-width: 88%; height: auto;" />
            <figcaption style="display: inline-block; text-align: left; font-size: 1.0em; color: #555; margin-top: 8px; max-width: 988px;">
                Effect of <strong>backbone architecture</strong> and <strong>model scaling</strong> on VLM performance. This table reports <em>Tolerance-based Accuracy</em> (<strong>↑</strong>) across different model variants of LLaVA-v1.6. The first two rows compare the impact of different backbone architectures (Mistral-7B vs. Vicuna-7B). The last three rows examine the effect of model scaling.  
            </figcaption>
        </figure>
        <div style="height: 16px;"></div>

        <figure style="text-align: center;">
            <img src="./static/images/table4.png" alt="Ablation study on CoT." style="max-width: 49%; height: auto;" />
            <figcaption style="display: inline-block; text-align: left; font-size: 1.0em; color: #555; margin-top: 8px; max-width: 988px;">
            Effects of <b>CoT reasoning</b> on VLM performance in NuScenes-SpatialQA.         
            </figcaption>
        </figure>


      </div>
    </div>
  </div>
</section>
<!-- End image carousel -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>